{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0d26dd2",
   "metadata": {},
   "source": [
    "# Catch Me If You Can (\"Alice\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0a2d9b",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c43a9d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_selection import chi2, SelectKBest, SelectFromModel\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "212ff467",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_cols = ['time' + str(i) for i in range(1, 11)]\n",
    "df_train = pd.read_csv('datasets/alice/train_sessions.csv', index_col='session_id', parse_dates=time_cols)\n",
    "df_test = pd.read_csv('datasets/alice/test_sessions.csv', index_col='session_id', parse_dates=time_cols)\n",
    "\n",
    "df_train.sort_values(by='time1', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfab131d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>time5</th>\n",
       "      <th>...</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21669</th>\n",
       "      <td>56</td>\n",
       "      <td>2013-01-12 08:05:57</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2013-01-12 08:05:57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54843</th>\n",
       "      <td>56</td>\n",
       "      <td>2013-01-12 08:37:23</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2013-01-12 08:37:23</td>\n",
       "      <td>56.0</td>\n",
       "      <td>2013-01-12 09:07:07</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2013-01-12 09:07:09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77292</th>\n",
       "      <td>946</td>\n",
       "      <td>2013-01-12 08:50:13</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:14</td>\n",
       "      <td>951.0</td>\n",
       "      <td>2013-01-12 08:50:15</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:15</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>784.0</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>949.0</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114021</th>\n",
       "      <td>945</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>949.0</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>945.0</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>947.0</td>\n",
       "      <td>2013-01-12 08:50:19</td>\n",
       "      <td>945.0</td>\n",
       "      <td>2013-01-12 08:50:19</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:19</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146670</th>\n",
       "      <td>947</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>950.0</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>947.0</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>950.0</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>951.0</td>\n",
       "      <td>2013-01-12 08:50:22</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:22</td>\n",
       "      <td>947.0</td>\n",
       "      <td>2013-01-12 08:50:22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1               time1  site2               time2  site3  \\\n",
       "session_id                                                                \n",
       "21669          56 2013-01-12 08:05:57   55.0 2013-01-12 08:05:57    NaN   \n",
       "54843          56 2013-01-12 08:37:23   55.0 2013-01-12 08:37:23   56.0   \n",
       "77292         946 2013-01-12 08:50:13  946.0 2013-01-12 08:50:14  951.0   \n",
       "114021        945 2013-01-12 08:50:17  948.0 2013-01-12 08:50:17  949.0   \n",
       "146670        947 2013-01-12 08:50:20  950.0 2013-01-12 08:50:20  948.0   \n",
       "\n",
       "                         time3  site4               time4  site5  \\\n",
       "session_id                                                         \n",
       "21669                      NaT    NaN                 NaT    NaN   \n",
       "54843      2013-01-12 09:07:07   55.0 2013-01-12 09:07:09    NaN   \n",
       "77292      2013-01-12 08:50:15  946.0 2013-01-12 08:50:15  946.0   \n",
       "114021     2013-01-12 08:50:18  948.0 2013-01-12 08:50:18  945.0   \n",
       "146670     2013-01-12 08:50:20  947.0 2013-01-12 08:50:21  950.0   \n",
       "\n",
       "                         time5  ...               time6  site7  \\\n",
       "session_id                      ...                              \n",
       "21669                      NaT  ...                 NaT    NaN   \n",
       "54843                      NaT  ...                 NaT    NaN   \n",
       "77292      2013-01-12 08:50:16  ... 2013-01-12 08:50:16  948.0   \n",
       "114021     2013-01-12 08:50:18  ... 2013-01-12 08:50:18  947.0   \n",
       "146670     2013-01-12 08:50:21  ... 2013-01-12 08:50:21  946.0   \n",
       "\n",
       "                         time7  site8               time8  site9  \\\n",
       "session_id                                                         \n",
       "21669                      NaT    NaN                 NaT    NaN   \n",
       "54843                      NaT    NaN                 NaT    NaN   \n",
       "77292      2013-01-12 08:50:16  784.0 2013-01-12 08:50:16  949.0   \n",
       "114021     2013-01-12 08:50:19  945.0 2013-01-12 08:50:19  946.0   \n",
       "146670     2013-01-12 08:50:21  951.0 2013-01-12 08:50:22  946.0   \n",
       "\n",
       "                         time9 site10              time10 target  \n",
       "session_id                                                        \n",
       "21669                      NaT    NaN                 NaT      0  \n",
       "54843                      NaT    NaN                 NaT      0  \n",
       "77292      2013-01-12 08:50:17  946.0 2013-01-12 08:50:17      0  \n",
       "114021     2013-01-12 08:50:19  946.0 2013-01-12 08:50:20      0  \n",
       "146670     2013-01-12 08:50:22  947.0 2013-01-12 08:50:22      0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26b49020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 253561 entries, 21669 to 204762\n",
      "Data columns (total 21 columns):\n",
      " #   Column  Non-Null Count   Dtype         \n",
      "---  ------  --------------   -----         \n",
      " 0   site1   253561 non-null  int64         \n",
      " 1   time1   253561 non-null  datetime64[ns]\n",
      " 2   site2   250098 non-null  float64       \n",
      " 3   time2   250098 non-null  datetime64[ns]\n",
      " 4   site3   246919 non-null  float64       \n",
      " 5   time3   246919 non-null  datetime64[ns]\n",
      " 6   site4   244321 non-null  float64       \n",
      " 7   time4   244321 non-null  datetime64[ns]\n",
      " 8   site5   241829 non-null  float64       \n",
      " 9   time5   241829 non-null  datetime64[ns]\n",
      " 10  site6   239495 non-null  float64       \n",
      " 11  time6   239495 non-null  datetime64[ns]\n",
      " 12  site7   237297 non-null  float64       \n",
      " 13  time7   237297 non-null  datetime64[ns]\n",
      " 14  site8   235224 non-null  float64       \n",
      " 15  time8   235224 non-null  datetime64[ns]\n",
      " 16  site9   233084 non-null  float64       \n",
      " 17  time9   233084 non-null  datetime64[ns]\n",
      " 18  site10  231052 non-null  float64       \n",
      " 19  time10  231052 non-null  datetime64[ns]\n",
      " 20  target  253561 non-null  int64         \n",
      "dtypes: datetime64[ns](10), float64(9), int64(2)\n",
      "memory usage: 42.6 MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed5a41d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "site1         0\n",
       "time1         0\n",
       "site2      3463\n",
       "time2      3463\n",
       "site3      6642\n",
       "time3      6642\n",
       "site4      9240\n",
       "time4      9240\n",
       "site5     11732\n",
       "time5     11732\n",
       "site6     14066\n",
       "time6     14066\n",
       "site7     16264\n",
       "time7     16264\n",
       "site8     18337\n",
       "time8     18337\n",
       "site9     20477\n",
       "time9     20477\n",
       "site10    22509\n",
       "time10    22509\n",
       "target        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f9b10e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((253561, 21), (82797, 20))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afc2bc8",
   "metadata": {},
   "source": [
    "In this particular problem, the missing values on columns site and time, e.g, site7 and time7, it means the user didn't visit a 7th website. In other words, only visited 6 websites.\n",
    "\n",
    "We'll fill missing values with a value of 0. For the moment, we'll work only with site features, then we apply feature engineering-selection to get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2ba2245",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['site' + str(i) for i in range(1, 11)] # ['site1', 'site2', 'site3', 'site4'...]\n",
    "\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "train_imputed = imputer.fit_transform(df_train[columns])\n",
    "test_imputed = imputer.transform(df_test[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be87031f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((253561, 10), (82797, 10))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_imputed.shape, test_imputed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92fc55ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 56.,  55.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [ 56.,  55.,  56.,  55.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [946., 946., 951., 946., 946., 945., 948., 784., 949., 946.],\n",
       "       [945., 948., 949., 948., 945., 946., 947., 945., 946., 946.],\n",
       "       [947., 950., 948., 947., 950., 952., 946., 951., 946., 947.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_imputed[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f10ab41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imputed = train_imputed.astype('int')\n",
    "test_imputed = test_imputed.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c37f5ef",
   "metadata": {},
   "source": [
    "We're going to use `CountVectorizer` to make a vocabulary of sites and count their apperance in each sample.\n",
    "\n",
    "1) We might save both datasets train and test imputed in documents for CountVectorizer processing.\n",
    "\n",
    "2) We might convert both datasets into str and fit CountVectorizer with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff65e836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1)\n",
    "# np.savetxt('datasets/alice/train_sessions_test.txt', train_imputed, delimiter=' ')\n",
    "# np.savetxt('datasets/alice/test_sessions_test.txt', test_imputed, delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47ffeeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2)\n",
    "train_imputed = [\" \".join(row.astype('str')) for row in train_imputed]\n",
    "test_imputed = [\" \".join(row.astype('str')) for row in test_imputed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "681fea81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.6 s, sys: 232 ms, total: 13.8 s\n",
      "Wall time: 13.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "counter = CountVectorizer(ngram_range=(1, 3), max_features=50000)\n",
    "X_train = counter.fit_transform(train_imputed)\n",
    "X_test = counter.transform(test_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f126c65b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((253561, 50000), (82797, 50000))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7d4f45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(253561,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = df_train['target'].astype('int').ravel()\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4329c076",
   "metadata": {},
   "source": [
    "Now, we can start building a machine learning model, however we can just do cross-validation as usual (`KFold` or another class) but as we're working with time series (even if we haven't added time features yet, remember that we ordered `df_train` by `time1`) we need to perform splitting in other way. Sklearn give us a class called `TimeSeriesSplit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2f75454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 60.9 ms, sys: 112 ms, total: 173 ms\n",
      "Wall time: 30.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "time_split = TimeSeriesSplit(n_splits=10)\n",
    "log_reg = LogisticRegression(solver='liblinear', random_state=42)\n",
    "cv_auc_log = cross_val_score(log_reg, X_train, y_train, cv=time_split, n_jobs=-1, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6193155f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8677191954657827, 0.08472558562304884)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_auc_log.mean(), cv_auc_log.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e42503fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.8 s, sys: 3.54 s, total: 52.4 s\n",
      "Wall time: 23 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=42, solver='liblinear')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1617e848",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob_log = log_reg.predict_proba(X_test)[:, 1] # [n_samples, n_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0641ab8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission_csv(pred, path_csv, id_col='session_id', target_col='target'):\n",
    "    submit_df = pd.DataFrame(pred, columns=[target_col], index=np.arange(1, pred.shape[0] + 1))\n",
    "    submit_df.rename_axis(id_col, inplace=True)\n",
    "    submit_df.to_csv(path_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d98f004",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_csv(y_pred_prob_log, 'datasets/alice/submit1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2811535",
   "metadata": {},
   "source": [
    "base log reg model\n",
    "\n",
    "cv (time series split) roc auc -> mean = 0.867 ; std = 0.084\n",
    "\n",
    "Public Leaderboard = 0.91288"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962072b0",
   "metadata": {},
   "source": [
    "Let's add time features but only the hour, if is in morning, day, afternoon and night. Why only this data? Because maybe there's no need to know the month or year when a user visited a website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c01e1cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time_features(X_, df_):\n",
    "    # time_cols is defined at the beginning of the notebook, just before import csv train/test\n",
    "    dict_time = {}\n",
    "        \n",
    "    for col in time_cols:\n",
    "        dict_time[col+'_hour'] = [time.hour for time in df_[col]]\n",
    "        dict_time[col+'_is_morning'] = [1 if (hour >= 5 and hour < 12) else 0 for hour in dict_time[col+'_hour']]\n",
    "        dict_time[col+'_is_afternoon']= [1 if (hour >= 12 and hour < 17) else 0 for hour in dict_time[col+'_hour']]\n",
    "        dict_time[col+'_is_evening'] = [1 if (hour >= 17 and hour < 21) else 0 for hour in dict_time[col+'_hour']]\n",
    "        dict_time[col+'_is_night'] = [1 if (hour >= 21 and hour < 5) else 0 for hour in dict_time[col+'_hour']]\n",
    "    \n",
    "    df_time = pd.DataFrame(dict_time).fillna(0)\n",
    "    return hstack([X_, df_time])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0ca0575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.5 s, sys: 492 ms, total: 14 s\n",
      "Wall time: 14.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_2 = extract_time_features(X_train, df_train)\n",
    "X_test_2 = extract_time_features(X_test, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49fab149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 128 ms, sys: 64 ms, total: 192 ms\n",
      "Wall time: 53.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9152897189426366, 0.054716865679914614)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "log_reg_2 = LogisticRegression(solver='sag', random_state=42)\n",
    "cv_auc_log_2 = cross_val_score(log_reg_2, X_train_2, y_train, cv=time_split, n_jobs=-1, scoring='roc_auc')\n",
    "cv_auc_log_2.mean(), cv_auc_log_2.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10c0a679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 0.8677191954657827 0.08472558562304884\n",
      "Now: 0.9152897189426366 0.054716865679914614\n"
     ]
    }
   ],
   "source": [
    "print(\"Before:\",cv_auc_log.mean(), cv_auc_log.std())\n",
    "print(\"Now:\",cv_auc_log_2.mean(), cv_auc_log_2.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de56717",
   "metadata": {},
   "source": [
    "Pog!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8fa5883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.2 s, sys: 21.5 ms, total: 23.2 s\n",
      "Wall time: 23.2 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lel/Escritorio/ml-projects/mlcourse/env/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=42, solver='sag')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "log_reg_2.fit(X_train_2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da0d92e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob_log_2 = log_reg_2.predict_proba(X_test_2)[:, 1]\n",
    "\n",
    "submission_csv(y_pred_prob_log_2, 'datasets/alice/submit2.1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ee69c7",
   "metadata": {},
   "source": [
    "Public Leaderboard = 0.93124"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d42cb49",
   "metadata": {},
   "source": [
    "Well, it's time to feature selection and then hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd8c1ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_feature_importances(coefs, columns, abs_=False, normalize=False, ascending_=False):\n",
    "    #flatten() returns a copy, so there's no need for set the parameter copy=True \n",
    "    df = pd.DataFrame(data=coefs.flatten(), columns=['importance'], index=columns) \n",
    "    if(abs_):\n",
    "        df['importance'] = np.abs(df['importance'])\n",
    "    if(normalize):\n",
    "        df['importance'] = (df['importance']-min(df['importance']))/(max(df['importance'])-min(df['importance']))\n",
    "    \n",
    "    df.sort_values(by='importance', ascending=ascending_, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70d5fb44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29 1600</th>\n",
       "      <td>5.511434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678 775 678</th>\n",
       "      <td>3.446028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3846 3847 617</th>\n",
       "      <td>3.313141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34632 11106 11106</th>\n",
       "      <td>3.153049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30111</th>\n",
       "      <td>2.925741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76 1057 76</th>\n",
       "      <td>0.178418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39 22 570</th>\n",
       "      <td>0.177864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145 52</th>\n",
       "      <td>0.177834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186 1939</th>\n",
       "      <td>0.177215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27816</th>\n",
       "      <td>0.176705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   importance\n",
       "29 1600              5.511434\n",
       "678 775 678          3.446028\n",
       "3846 3847 617        3.313141\n",
       "34632 11106 11106    3.153049\n",
       "30111                2.925741\n",
       "...                       ...\n",
       "76 1057 76           0.178418\n",
       "39 22 570            0.177864\n",
       "145 52               0.177834\n",
       "186 1939             0.177215\n",
       "27816                0.176705\n",
       "\n",
       "[3000 rows x 1 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_feature_importances(log_reg.coef_, counter.vocabulary_)[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a04ced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "selectk = SelectKBest(chi2, k=40000)\n",
    "X_train_3 = selectk.fit_transform(X_train_2, y_train)\n",
    "X_test_3 = selectk.transform(X_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c663b6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55.7 ms, sys: 51.9 ms, total: 108 ms\n",
      "Wall time: 44.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9158598630917517, 0.05455492654126366)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "log_reg_3 = LogisticRegression(solver='sag', random_state=42)\n",
    "cv_auc_log_3 = cross_val_score(log_reg_3, X_train_3, y_train, cv=time_split, n_jobs=-1, scoring='roc_auc')\n",
    "cv_auc_log_3.mean(), cv_auc_log_3.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0cde8cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.1 s, sys: 19.8 ms, total: 21.1 s\n",
      "Wall time: 21.3 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lel/Escritorio/ml-projects/mlcourse/env/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=42, solver='sag')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "log_reg_3.fit(X_train_3, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83e98aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob_log_3 = log_reg_3.predict_proba(X_test_3)[:, 1]\n",
    "submission_csv(y_pred_prob_log_3, 'datasets/alice/submit3.1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad04b77",
   "metadata": {},
   "source": [
    "Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "153921fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_values = np.logspace(-2, 2, 10)\n",
    "\n",
    "log_reg_grid = GridSearchCV(log_reg_3, param_grid={'C': c_values}, scoring='roc_auc', n_jobs=-1,\n",
    "                            cv=time_split, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "45bdb4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "CPU times: user 1min 2s, sys: 3.79 s, total: 1min 5s\n",
      "Wall time: 9min 3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=10, test_size=None),\n",
       "             estimator=LogisticRegression(random_state=42, solver='liblinear'),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'C': array([1.00000000e-02, 2.78255940e-02, 7.74263683e-02, 2.15443469e-01,\n",
       "       5.99484250e-01, 1.66810054e+00, 4.64158883e+00, 1.29154967e+01,\n",
       "       3.59381366e+01, 1.00000000e+02])},\n",
       "             scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "log_reg_grid.fit(X_train_3, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ef7660c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9208102253865565, {'C': 0.21544346900318834})"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg_grid.best_score_, log_reg_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6808afc",
   "metadata": {},
   "source": [
    "solver='liblinear' very pog :0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8fdb90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_log_reg_3 = LogisticRegression(C=0.21544346900318834 , solver='liblinear', random_state=42)\n",
    "tuned_log_reg_3.fit(X_train_3, y_train)\n",
    "tuned_y_pred_prob_log_3 = tuned_log_reg_3.predict_proba(X_test_3)[:, 1]\n",
    "submission_csv(tuned_y_pred_prob_log_3, 'datasets/alice/submit3_tuned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdd1c44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cddf6b82",
   "metadata": {},
   "source": [
    "**Maybe this is cheating but...I watched some public kernels about \"catch me if you can\" :'v. I know more or less what I need to do, but, I'm going to do it in my way.**\n",
    "\n",
    "PD: I don't now why it doesn't work :'d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4825c65c",
   "metadata": {},
   "source": [
    "Changes:\n",
    "* I'll try to write code cleaner\n",
    "* New features from time (session time start-end, lengh session, day of week, number of visited sites)\n",
    "* Remove some created features (time2 to time10, morning, afternoon, etc.)\n",
    "\n",
    "Let's try to organize better all code above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f337d61",
   "metadata": {},
   "source": [
    "**Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44b01131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_time(df):\n",
    "    new_features = {}\n",
    "    new_features['start_session'] = df['time1'] # np.min(df.loc[:, time_cols], axis=1)\n",
    "    new_features['num_sites'] = np.sum(df.loc[:, site_cols].notna(), axis=1)\n",
    "    new_features['day_of_week'] = [date.day_of_week for date in new_features['start_session']]\n",
    "    new_features['day_in_month'] = [date.days_in_month for date in new_features['start_session']]\n",
    "    new_features['day_of_year'] = [date.day_of_year for date in new_features['start_session']]\n",
    "    new_features['hour_session'] = [date.hour for date in new_features['start_session']]\n",
    "    \n",
    "#     new_features['is_morning'] = [1 if (hour >= 5 and hour < 12) else 0 for hour in new_features['hour_session']]\n",
    "#     new_features['is_afternoon']=[1 if (hour >= 12 and hour < 17) else 0 for hour in new_features['hour_session']]\n",
    "#     new_features['is_evening'] = [1 if (hour >= 17 and hour < 21) else 0 for hour in new_features['hour_session']]\n",
    "#     new_features['is_night'] = [1 if (hour >= 21 and hour < 5) else 0 for hour in new_features['hour_session']]\n",
    "    \n",
    "    df_features = pd.DataFrame(new_features)\n",
    "    df_features.sort_values(by='start_session', inplace=True)\n",
    "    df_features.drop(['start_session'], axis=1, inplace=True)   \n",
    "    return df_features\n",
    "\n",
    "def roc_auc_cv_log_reg(X_train, y_train, cv):\n",
    "    \"\"\"\n",
    "    # default values\n",
    "    \n",
    "    LogisticRegression model -> solver='liblinear', random_state=42 \n",
    "    cross_val_score -> n_jobs=-1, scoring='roc_auc' \n",
    "    \n",
    "    Returns the mean and std of cross-validation scores.\n",
    "    \"\"\"\n",
    "    log_reg = LogisticRegression(solver='liblinear', random_state=42)\n",
    "    y_pred_cv = cross_val_score(log_reg, X_train, y_train, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "    return y_pred_cv.mean(), y_pred_cv.std()\n",
    "\n",
    "def train_predict_proba_log_reg(X_train, y_train, X_test):\n",
    "    \"\"\"\n",
    "    # default values\n",
    "    \n",
    "    LogisticRegression model -> solver='liblinear', random_state=42 \n",
    "    .predict_proba(X_test)[:, 1] # all rows in the second column\n",
    "    \n",
    "    Returns the probabilities of each sample belongs to a specific class\n",
    "    \"\"\"\n",
    "    log_reg = LogisticRegression(solver='liblinear', random_state=42)\n",
    "    log_reg.fit(X_train, y_train)\n",
    "    y_predict_proba = log_reg.predict_proba(X_test)[:, 1]\n",
    "    return y_predict_proba\n",
    "\n",
    "def submission_csv(pred, path_csv, id_col='session_id', target_col='target'):\n",
    "    submit_df = pd.DataFrame(pred, columns=[target_col], index=np.arange(1, pred.shape[0] + 1))\n",
    "    submit_df.rename_axis(id_col, inplace=True)\n",
    "    submit_df.to_csv(path_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96831f8",
   "metadata": {},
   "source": [
    "**Preparing the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9afb99fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'datasets/alice/'\n",
    "\n",
    "site_cols = ['site' + str(i) for i in range(1, 11)] # ['site1', 'site2', 'site3', 'site4'...]\n",
    "time_cols = ['time' + str(i) for i in range(1, 11)]\n",
    "\n",
    "df_train = pd.read_csv(DATASET_PATH+'train_sessions.csv', index_col='session_id', parse_dates=time_cols)\n",
    "df_test = pd.read_csv(DATASET_PATH+'test_sessions.csv', index_col='session_id', parse_dates=time_cols)\n",
    "\n",
    "df_train.sort_values(by='time1', inplace=True)\n",
    "\n",
    "# Fill missing values\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "train_imputed = imputer.fit_transform(df_train[site_cols]).astype('int')\n",
    "test_imputed = imputer.transform(df_test[site_cols]).astype('int')\n",
    "\n",
    "train_imputed = [\" \".join(row.astype('str')) for row in train_imputed]\n",
    "test_imputed = [\" \".join(row.astype('str')) for row in test_imputed]\n",
    "\n",
    "# Get vocabulary of sites\n",
    "counter = CountVectorizer(ngram_range=(1, 3), max_features=50000)\n",
    "X_train = counter.fit_transform(train_imputed)\n",
    "X_test = counter.transform(test_imputed)\n",
    "\n",
    "y_train = df_train['target'].astype('int').ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d3457e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scaling new features - doesn't work very much that not using it\n",
    "# scaler = StandardScaler()\n",
    "# X_train_features = scaler.fit_transform(extract_features_from_time(df_train))\n",
    "# X_test_features = scaler.transform(extract_features_from_time(df_test))\n",
    "\n",
    "X_train_features = extract_features_from_time(df_train)\n",
    "X_test_features = extract_features_from_time(df_test)\n",
    "\n",
    "# Adding new features\n",
    "X_train_4 = hstack([X_train, X_train_features])\n",
    "X_test_4 = hstack([X_test, X_test_features])\n",
    "\n",
    "# Split for correct cross-validation\n",
    "time_split = TimeSeriesSplit(n_splits=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaa4ed8",
   "metadata": {},
   "source": [
    "**Building models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e0b8a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.912838504257361, 0.07848928412473063)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sin is_\n",
    "cv_auc_log_4_mean, cv_auc_log_4_std = roc_auc_cv_log_reg(X_train_4, y_train, time_split)\n",
    "cv_auc_log_4_mean, cv_auc_log_4_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30acd211",
   "metadata": {},
   "source": [
    "#### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aad9713e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9173077343918777, 0.07008873248479301)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selectk = SelectKBest(chi2, k=40000)\n",
    "# X_train_5k = selectk.fit_transform(X_train_4, y_train)\n",
    "# X_test_5k = selectk.transform(X_test_4)\n",
    "\n",
    "# cv_auc_log_5k_mean, cv_auc_log_5k_std = roc_auc_cv_log_reg(X_train_5k, y_train, time_split)\n",
    "# cv_auc_log_5k_mean, cv_auc_log_5k_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1ddc95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_4 = LogisticRegression(solver='liblinear', random_state=42)\n",
    "log_reg_4.fit(X_train_4, y_train)\n",
    "\n",
    "y_pred_prob_log_4 = log_reg_4.predict_proba(X_test_4)[:, 1]\n",
    "\n",
    "submission_csv(y_pred_prob_log_4, 'datasets/alice/submit4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273062af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
